| metric | overall | temporal reasoning | open domain | multi hop | single hop | Unknown |
|---|---|---|---|---|---|---|
| llm_judge_score | 18.18 | 5.61 | 36.11 | 12.77 | 22.67 | # |
| llm_judge_std | 0.0008 | 0.0000 | 0.0049 | 0.0029 | 0.0020 | # |
| f1 | 11.74 | 11.45 | 19.74 | 8.63 | 12.01 | 0.00 |
| rouge1_f | 12.64 | 11.62 | 20.76 | 8.92 | 13.38 | 0.00 |
| rouge2_f | 2.43 | 1.09 | 5.03 | 1.45 | 2.98 | 0.00 |
| rougeL_f | 12.24 | 11.58 | 20.56 | 8.50 | 12.82 | 0.00 |
| bleu1 | 8.84 | 8.82 | 14.28 | 5.78 | 9.27 | 0.00 |
| bleu2 | 3.93 | 3.43 | 7.21 | 2.24 | 4.32 | 0.00 |
| bleu3 | 2.62 | 2.27 | 4.47 | 1.59 | 2.89 | 0.00 |
| bleu4 | 2.16 | 1.88 | 3.56 | 1.36 | 2.38 | 0.00 |
| meteor | 8.94 | 8.69 | 14.05 | 6.17 | 9.40 | 0.00 |
| bert_f1 | 19.18 | 20.56 | 26.48 | 13.52 | 19.71 | # |
| similarity | 53.32 | 49.51 | 59.53 | 52.00 | 54.46 | # |

Unknown 一栏出现了数据错误。但是我复查结果，发现所有问题都选择了“not mentioned in the conversation”，即回答全部错误。人工判断得分应该为0。

## Context Token
对于每个QA，我都上传sample中所有对话作为context。因此，如果一个sample有100个QA，那么整段对话就被上传了100遍. 因此我们只需要计算每个sample全部conversation的context token：  

```
1: 25338
2: 19678
3: 37498
4: 32972
5: 38617
6: 38363
7: 35880
8: 34846
9: 28281
10: 34374
```

## time information
| metric | overall | temporal reasoning | open domain | multi hop | single hop | Unknown |
|---|---|---|---|---|---|---|
| response_duration_ms | 22791.2282 | 29318.2352 | 30665.1615 | 29492.3123 | 17192.8224 | 6534.6371 |
| response_duration_ms_p50 | 22163.1260 | 29736.1731 | 30161.7692 | 29369.0090 | 16565.5150 | 6534.6371 |
| response_duration_ms_p95 | 38133.3028 | 40708.7169 | 41049.5350 | 40345.2854 | 27704.8330 | 7483.2118 |
