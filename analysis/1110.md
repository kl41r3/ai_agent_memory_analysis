### analysis

amem没有分session的区别。因此，可以看作single 和 multi 在处理上没有区别。但是，single-session的问题是真的只提供了一个session给llm分析，所以差异是可能数据量少一点。

`single-session-user` 是针对user情况的提问，比如“where do I take yoga classes?”，回答则是非常简单的地名、人名、机构名称等信息。

而`single-sesssion-assistant`则是模仿人类用户的情境提问，llm需要作为assistant回答。如"I'm checking our previous chat about the shift rotation sheet for GM social media agents. Can you remind me what was the rotation for Admon on a Sunday?"，回答"Admon was assigned to the 8 am - 4 pm (Day Shift) on Sundays."

我分别测试了`single-session-user/assistant/preference`，发现user召回的context几乎和问题都不相关。可以看出llm回答准确度和context的质量直接挂钩。

那么接下来自然的问题是：为什么`user`的问题分数反而更低呢？

我让llm重新生成记忆文件，回答准确度上升了很多。原来三个全部牛头不对马嘴，现在有两个可以一字不差地回答。看来是因为数据对话太少，而amem又没有热力学机制，导致回答直接被淹没在无穷的记忆里面。

amem是综合的记忆机制。那么如何改进？cashed_memory文件数量不对！oh，不，它不是！是我在跑的时候弄错了。因为cashed_memory是按照跑的顺序标号的。所以没能找到正确的指标！

那么反思locomo是否对了呢？从有10*3=30个文件推测，当时没断（和我的记忆一致）。所以没有问题。

问题归根结底出在了断点续跑的机制上，虽然可以简单地通过改写ratio来续跑，但是无法检测到正确的cashed_memory。接下来我们只要重跑amem on lme。


### Steps

下一步：demo里放想要测试的问题，比如single-session-user. 尝试从context 的角度解读为什么分数这么低。✅  
下一步：查看如果记忆重写还不行，那么问题出在哪里？如果没问题，那么如何改进记忆机制？为什么question间的记忆会打架？那么locomo的记忆会不会打架？✅  
下一步：重新跑amem测试。
下一步：跑研究下一个算法，改写并跑测试。



3-user  
4-assistant  
5-preference  
6-user记忆重新书写，从0开始再来回答问题  