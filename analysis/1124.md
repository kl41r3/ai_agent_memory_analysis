Next Step：
1. 重跑MemoryOS，关注multi-session和single-session-preference。看看为什么llm judge给出的评分和metrics对不上？
2. 跑llm direct on locomo (根据context token的限制上传对话。。研究一下)
3. letta（找到一个比较好的版本然后debug然后跑。。。）



## example of single-session-preference
``` json
{
    "question": "Can you recommend some recent publications or conferences that I might find interesting?",
    "system_answer": "- **Conferences**: \n  - Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2023\n  - International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2023\n  - IEEE International Symposium on Biomedical Imaging (ISBI) 2023 \n\n- **Publications**: \n  - \"Explainable AI in Medical Imaging: A Review\" (2023)\n  - \"Advancements in Deep Learning for Medical Image Analysis\" (2023)\n  - \"Interpretability of Deep Learning Models in Medical Imaging\" (2023)",
    "original_answer": "The user would prefer suggestions related to recent research papers, articles, or conferences that focus on artificial intelligence in healthcare, particularly those that involve deep learning for medical image analysis. They would not be interested in general AI topics or those unrelated to healthcare.",
}
```
如图所示，llm确实按照preference直接回答了问题。`original_answer`只是给出了回答的方向，而`system_answer`则是具体的回答。难怪metrics数值低而llm_judge 得分高。


## 分析memories，multi-session的结果

根据3个样本的结果和返回的context，我认为llm_judge可能存在误判的情况。multi-session属于填空题而并非简答题，出现了大量遗漏情况。

于是我重新跑了所有的multi-session问题。本次结果llm_judge从之前的87.1来到了现在的56.6，将近下降了30，而metrics结果都略上升几个点，差异不大。这让我深度怀疑llm_judge的结果可靠性。

## direct on locomo

结果非常糟糕！因为locomo的一个sample对话量比lme大很多。

最小学生⬇️
``` json
{
      "question": "What is Caroline's identity?",
      "answer": "Caroline is a speaker in the conversation.",
      "category": 1,
      "golden_answer": "Transgender woman",
      "search_context": "",
      "response_duration_ms": 4364.940881729126,
      "search_duration_ms": 0
    },
```

当然，最有意思的还是adversarial question全错。llm偷大懒！

## letta

本质上是一个有记忆加强能力的agent。问题在于，它默认的记忆存储模式是，用户发消息，它回答，这样形成了记忆内容。而数据集的模式，在letta的语境下会被识别为一个document。（document分析能力也是letta有在论文提到过，它表现不错的地方）。那我要如何处理？

我想了一个办法，是把数据集中的对话直接按照latta的方式，处理好为记忆文件（LocalStateManager），然后在输入问题，让letta回答，记录结果。