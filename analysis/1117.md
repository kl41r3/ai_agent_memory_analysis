### analysis

Next Step：
1. 重跑MemoryOS，关注multi-session和single-session-preference。看看为什么llm judge给出的评分和metrics对不上？
2. 跑llm direct on locomo
3. letta




### 问题：
* 测试的时间难道不是和网速关联很大吗？这还能准确吗？  
* 是否用4.1再跑一下直接测试数据集的结果？


### amem程序写得非常粗糙但是我没必要（懒得）改的地方：  
1. 退出机制不够优雅。本来思路是直接写入文件防止报错跑了一半的结果丢失，现在想来应该模仿其他的算法当中，正常保存在字典，用try-exception机制，如果错误就是直接先保存，再退出，同时给出错误堆栈信息。amem的作者是开发地太顺利了吗，怎么没有断点续跑，记忆文件直接按照顺序命名，然后也没有好的退出机制？？
2. 进度条写废了。。之前应该是locomo一个sample qa的进度条现在由于格式问题直接变成永远只有一个问题要么0要么百分百的进度条了。。

### 用llm直接跑数据集
我基于memos，开发了一个直接通过llm测试lme数据集的程序。结果可见。

关于locomo：  
所以context要怎么写才好？还有就是category5 adversarial question没有被测量。我要自己添加prompt(比如参考amem)来测试category5吗？

### analysis of memoryos, amem and direct llm on LME
为什么MemoryOS的single-session-preference 这么高？明明下面的指标都很低。。因为之前结果文件丢失我现在也看不到答案。。    

single-session-user/assistant分数，两个算法都比直接跑的低。amem略低但差不了多少，memoryos低得可怕。或者说，对于这种，问题，在lme这样感觉对话不是天文数字低时候，llm的发挥不会差，所以要对于locomo跑出来的结果再做比较。有关single-session-preference，llm直接发挥果然差点意思，这种问题是算法主要提升的地方。 

knowledge-update，llm直接发挥居然超过了两个算法。这个值得研究。理论上讲，这样的问题应该是算法通过记忆的整理发挥优势的地方。我不太能理解为什么算法输了。

temporal-reasoning, memoryos终于略胜amem，但是也仅仅和llm直接发挥效果差不多。

multi-session，amem发挥不行。amem的优势还是在数据量处理比较少的时候。但是我还是觉得memoryos的数据很诡异，metrics和llm—judge对不上。我还需要进一步分析。

总体而言，我觉得这样看来llm本身的能力才非常关键……确实要尝试gpt-4.1。

### 写代码的心得
感觉现在更加能知道好的、清晰的、易于交流的代码是什么样的。我现在看我之前魔改的amem和memoryos觉得非常丑陋。