# Analysis on A-mem and MemoryOS, on LoCoMo and LME

Memoryos outperforms amem overall.

## overall analysis

### amem的优势在knowledge-update 和 context token 
amem 在knowledge update 上表现良好，优于memoryos。这也是唯一可以与memoryos相比较的指标。另一个优势在于context token更少，这也是amem的一个特色（amem 4k+, memoryos 5k+）。我推测是因为amem更加灵活的link和evolve机制。memoryos在这一点更加机械化，但反之，也更加可靠。这让我想起人脑。amem的算法无疑是更“human”的，humam更节能，但是也更不精确。这一点运用了神经科学的思想。

### lme的single问题难度更高

lme数据集的single 问题给两种算法都造成了困扰。和locomo数据集的single问题相比，LME的single问题难度整体更大一点。两个算法都表现出了分数的下降。locomo 的single分数很好拿，而lme的single问题竟然成了分数比较低的问题。这些问题到底难在哪里呢？暂时我看不出来。这个问题值得探究。

在single-session-user，single-session-assistant 和single-session-preference 三者之间，preference的分数是最高的。这是一个有意思的现象。我看到amem和memoryos的 single-session-user 分数都比 single-session-assistant 要低，这是为什么造成的？在locomo中二者身份是等同的。我推测是二者说话的方式不同，导致的最后召回答案的难度不同。

### single 和 multi 的比较 （为什么 multi 普遍更高？）

lme：  
- memoryos: metrics single > multi. llm as a judge: single < multi   **(inconcsistent)**  
- amem: metrics: single < multi. llm as a judge: single < multi  **(consistent)**  

Why this inconcsistency appears? multi的回答复杂度更高，更“简答题”。需要多步推理的时候，llm倾向于把推理的一部分写出来，导致最终metrics低。见下面对于llm as a judge 的分析。应该普遍相信，llm的判断更可靠，两者分数都是 multi > single。


对于locomo，amem 没有llm as a judge 的分数。但是就连metrics，都可以看出multi略比single高。而memoryos，却single碾压multi。

这一点我不太能理解。理论来说，multi的难度应该是更高的。这原因是什么？

### amem表现极其差

我不知道为什么amem在single-session-user，single-session-assistant和single-session-preference上的评分这么低。我认为这和locomo的single-hop应该是一致的问题。我特地跑回去检查了一下，代码没有什么明显问题，category的对应是正确的。没有log文件，我无法查看当时的context调回是否合理，这是关键的一步。那么我推测可能是memory在调回的时候出现了巨大的问题。有时候LLM给出了“I'm AI I have no preference”，也有很多"not enough information"这样的回答，显然是context没有给出正确和足够的信息。为什么原来single-hop的问题在这里却失灵了？这需要更多的记录和测试。

这个amem的分数有点太低了，尤其是single部分，我还是忍不住怀疑是否是我的代码改坏了。有没有别人跑的amem on LME 的结果我可以参考一下？或者我想单独再测试一下single的问题部分，查看一下context的返回情况。




``` python
# promnpt of memoryos, which is important for llm to return effective information

f"For questions that require answering a date or time, strictly follow the format \"15 July 2023\" and provide a specific date whenever possible. For example, if you need to answer \"last year,\" give the specific year of last year rather than just saying \"last year.\" Only provide one year, date, or time, without any extra responses.\n"
```

## llm as a judge

single-session-preference 的llm judge和metrics的分数差异比较大，因为preference更多是简答题，而其他问题更像有明确答案的填空题。在类似“简答式”回答中，llm能有更客观的判断。接下来看这个例子：

``` json
 "60036106": {
        "user_id": "60036106",
        "category": "multi-session",
        "question": "What was the total number of people reached by my Facebook ad campaign and Instagram influencer collaboration?",
        "question_date": "2023/05/30 (Tue) 20:55",
        "golden_answer": "12,000",
        "answer": "2,000 people from Facebook ad campaign and 10,000 from Instagram influencer collaboration.",
        "llm_judgments": {
            "judgment_1": true,
            "judgment_2": true,
            "judgment_3": true
        },
        "nlp_metrics": {
            "context_tokens": 4111,
            "lexical": {
                "f1": 0.0,
                "rouge1_f": 0.125,
                "rouge2_f": 0.0,
                "rougeL_f": 0.125,
                "bleu1": 0,
                "bleu2": 0,
                "bleu3": 0,
                "bleu4": 0,
                "meteor": 0.0
            },
            "semantic": {
                "similarity": 0.3072450848592845,
                "bert_f1": 0.2845969796180725
            }
        },
```

这是amem on lme 的某个回答。像这种情况，显然也应该使用llm直接作为judge，这个点应该被重视。尤其在测试阶段，如f1、bert、rogue这样的指标固然有道理，但是llm 作为judge的可靠性应该被估计。

所以我产生了一个对于这方面研究的想法：llm as a judge 是否值得被作为一种本研究领域内评估的新范式提出来？之前的各种metrics有其道理，但是记忆回溯方面，判断显然是更精准的好。